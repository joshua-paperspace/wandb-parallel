{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9728fae-bfa6-42b6-80a3-038bb55fc7af",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "e9728fae-bfa6-42b6-80a3-038bb55fc7af",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "# Weights & Biases with Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883566ec-a8d4-4275-8ade-8e6d5400475f",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "883566ec-a8d4-4275-8ade-8e6d5400475f",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "# Preface\n",
    "\n",
    "Weights and Biases is a ML Ops platform that has useful features around model tracking, hyperparameter tuning, and artifact saving during model training steps. Integrating with Weights and Biases provides Gradient users access to world-class model experimenting features while taking advantage of Gradient easy-to-use development platform and access to accelerated hardware.\n",
    "\n",
    "The goal of this tutorial is to highlight Weights and Biases features and how to use those within Gradient to scale up model training. During this tutorial you will learn to initiate W&B model runs, log metrics, save artifacts, tune hyperparameters, and determine the best performing model. The models trained during this tutorial can be saved in a Gradient Dataset and then be leveraged within Gradient Workflows and Deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ee5bc-0b15-49bd-afaf-c7332018886c",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "eb3ee5bc-0b15-49bd-afaf-c7332018886c",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "\n",
    "# Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec09825-b8c8-4e3b-b094-686b93c964e2",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "editing": false,
     "id": "2ec09825-b8c8-4e3b-b094-686b93c964e2",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48fc36-dbab-4d2b-a579-61fc8122dd20",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "9a48fc36-dbab-4d2b-a579-61fc8122dd20",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"./train_model_wandb.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c585a73f-8953-47d2-94f7-731f1a28204d",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c585a73f-8953-47d2-94f7-731f1a28204d",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "# Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f154991-d6c5-4544-b949-9ce02a36137d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": true,
     "id": "1f154991-d6c5-4544-b949-9ce02a36137d",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wandb.login(key='your-api-key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32c911-123d-40a2-a45c-473200b97a13",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "8e32c911-123d-40a2-a45c-473200b97a13",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "# Initalizing a Model Run and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea40ccc-0eb1-4638-8983-1e7fce90a891",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "aea40ccc-0eb1-4638-8983-1e7fce90a891",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "config={\n",
    "         \"epochs\": 5,\n",
    "         \"batch_size\": 128,\n",
    "         \"lr\": 1e-3,\n",
    "         \"model\": 'ResNet18'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329f15f-6b9f-439f-858f-0f978620df47",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "9329f15f-6b9f-439f-858f-0f978620df47",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "from resnet import resnet18, resnet34\n",
    "from load_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f772b1-3f33-4daa-ab3d-ef52b054b0ff",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "54f772b1-3f33-4daa-ab3d-ef52b054b0ff",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def validate_model(model, valid_dl, loss_func, device):\n",
    "    \n",
    "    # Compute performance of the model on the validation dataset\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        correct = 0\n",
    "        for i, (images, labels) in enumerate(valid_dl, 0):\n",
    "            \n",
    "            # Move data to GPU if available \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            val_loss += loss_func(outputs, labels)*labels.size(0)\n",
    "\n",
    "            # Compute accuracy and accumulate\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caacdd4-c8a7-4c19-968c-117ca42f8656",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1caacdd4-c8a7-4c19-968c-117ca42f8656",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "### Saving a model as a Gradient artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46634cf2-fcd1-4646-924a-dca081339ea0",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "46634cf2-fcd1-4646-924a-dca081339ea0",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install gradient -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71136ae9-9df2-4f14-86e2-a95ed2d6123f",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "71136ae9-9df2-4f14-86e2-a95ed2d6123f",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from gradient import ModelsClient\n",
    "\n",
    "models_client = ModelsClient(api_key='your-gradient-api-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f035c-42a9-404a-b193-f6e01b6a9380",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "092f035c-42a9-404a-b193-f6e01b6a9380",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def upload_model(config, model_client, model_dir='models'):\n",
    "\n",
    "    # Create model directory\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    # Save model file\n",
    "    params = [config['model'], 'epchs', str(config['epochs']), 'bs', str(config['batch_size']), 'lr', str(round(config['lr'], 6))]\n",
    "    full_model_name = '-'.join(params)\n",
    "    model_path = os.path.join(model_dir, full_model_name + '.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Upload model as a Gradient artifact\n",
    "    model_client.upload(path=model_path, name=full_model_name, model_type='Custom', project_id='your-project-id')\n",
    "\n",
    "    return full_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49898835-d09b-48fc-8cfb-221af48644b5",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "49898835-d09b-48fc-8cfb-221af48644b5",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "### Train ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df04398-e565-4b08-812b-d85c79046e90",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "6df04398-e565-4b08-812b-d85c79046e90",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'ResNet18'\n",
    "\n",
    "# Initialize W&B run\n",
    "with wandb.init(project=\"test-project\", config=config, name=model_name):\n",
    "\n",
    "    # Create Data Loader objects\n",
    "    trainloader, valloader, testloader = load_data(config)\n",
    "\n",
    "    # Create ResNet18 Model with 3 channel inputs (colored image) and 10 output classes\n",
    "    model = resnet18(3, 10)\n",
    "\n",
    "    # Define loss and optimization functions\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "\n",
    "    # Move the model to GPU if accessible \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    step = 0\n",
    "    epoch_durations = []\n",
    "    for epoch in range(config['epochs']):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        batch_checkpoint=50\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "            # Move data to GPU if available \n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + Backward + Optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Log every 50 mini-batches\n",
    "            if i % batch_checkpoint == batch_checkpoint-1:    # log every 50 mini-batches\n",
    "                step +=1\n",
    "                print(f'epoch: {epoch + ((i+1)/len(trainloader)):.2f}')\n",
    "                wandb.log({\"train_loss\": running_loss/batch_checkpoint, \"epoch\": epoch + ((i+1)/len(trainloader))}, step=step)\n",
    "            \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / batch_checkpoint))\n",
    "                    \n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Log validation metrics\n",
    "        val_loss, accuracy = validate_model(model, valloader, criterion, device)\n",
    "        wandb.log({\"val_loss\": val_loss, \"val_accuracy\": accuracy}, step=step)\n",
    "        print(f\"Valid Loss: {val_loss:3f}, accuracy: {accuracy:.2f}\")\n",
    "        \n",
    "        # Log epoch duration\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        wandb.log({\"epoch_runtime (seconds)\": epoch_duration}, step=step)\n",
    "\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "    # Log average epoch duration\n",
    "    avg_epoch_runtime = sum(epoch_durations) / len(epoch_durations)\n",
    "    wandb.log({\"avg epoch runtime (seconds)\": avg_epoch_runtime})\n",
    "\n",
    "    #Upload model artifact to Gradient and log model name to W&B\n",
    "    full_model_name = upload_model(config, model_client)\n",
    "    wandb.log({\"Notes\": full_model_name})\n",
    "\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ab816-312d-4aa6-b4b5-36c0b6482c58",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ba5ab816-312d-4aa6-b4b5-36c0b6482c58",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "### Train ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b9fc6-e027-4443-9197-42fe8fe109bf",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "editing": false,
     "id": "120b9fc6-e027-4443-9197-42fe8fe109bf",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'ResNet34'\n",
    "config['model'] = model_name\n",
    "\n",
    "# Initialize W&B run\n",
    "with wandb.init(project=\"test-project\", config=config, name=model_name):\n",
    "\n",
    "    # Create Data Loader objects\n",
    "    trainloader, valloader, testloader = load_data(config)\n",
    "\n",
    "    # Create ResNet34 Model with 3 channel inputs (colored image) and 10 output classes\n",
    "    model = resnet34(3, 10)\n",
    "\n",
    "    # Define loss and optimization functions\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "\n",
    "    # Move the model to GPU if accessible \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    step = 0\n",
    "    epoch_durations = []\n",
    "    for epoch in range(config['epochs']):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        batch_checkpoint=50\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "            # Move data to GPU if available \n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + Backward + Optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # log every 50 mini-batches\n",
    "            if i % batch_checkpoint == batch_checkpoint-1:    # log every 50 mini-batches\n",
    "                step +=1\n",
    "                print(f'epoch: {epoch + ((i+1)/len(trainloader)):.2f}')\n",
    "                wandb.log({\"train_loss\": running_loss/batch_checkpoint, \"epoch\": epoch + ((i+1)/len(trainloader))}, step=step)\n",
    "            \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / batch_checkpoint))\n",
    "                    \n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Log validation metrics\n",
    "        val_loss, accuracy = validate_model(model, valloader, criterion, device)\n",
    "        wandb.log({\"val_loss\": val_loss, \"val_accuracy\": accuracy}, step=step)\n",
    "        print(f\"Valid Loss: {val_loss:3f}, accuracy: {accuracy:.2f}\")\n",
    "        \n",
    "        # Log epoch duration\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        wandb.log({\"epoch_runtime (seconds)\": epoch_duration}, step=step)\n",
    "\n",
    "        epoch_durations.append(epoch_duration)\n",
    "\n",
    "    # Log average epoch duration\n",
    "    avg_epoch_runtime = sum(epoch_durations) / len(epoch_durations)\n",
    "    wandb.log({\"avg epoch runtime (seconds)\": avg_epoch_runtime})\n",
    "\n",
    "    #Upload model artifact to Gradient and log model name to W&B\n",
    "    full_model_name = upload_model(config, model_client)\n",
    "    wandb.log({\"Notes\": full_model_name})\n",
    "\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635c65c-702a-42e6-bdd6-24f2e742d897",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "b635c65c-702a-42e6-bdd6-24f2e742d897",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "# Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1d34a-d8dc-4eac-8b95-6c11417c6c9d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "42c1d34a-d8dc-4eac-8b95-6c11417c6c9d",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Classes of images in CIFAR-10 dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Initialize W&B run\n",
    "with wandb.init(project='test-project'):\n",
    "    # Create W&B artifact\n",
    "    artifact = wandb.Artifact('cifar10_image_predictions', type='predictions')\n",
    "\n",
    "    # Create Data Loader objects\n",
    "    trainloader, valloader, testloader = load_data(config)\n",
    "\n",
    "    # Create columns for W&B table\n",
    "    columns=['image', 'label', 'prediction']\n",
    "    for digit in range(10):\n",
    "        columns.append(\"score_\" + classes[digit])\n",
    "\n",
    "    # Create W&B table\n",
    "    pred_table = wandb.Table(columns=columns)   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "\n",
    "            # Move data to GPU if available \n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Calculate model outputs and predictions\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Loop through first batch of images and add data to the table\n",
    "            for j, image in enumerate(inputs, 0):\n",
    "                pred_table.add_data(wandb.Image(image), classes[labels[j].item()], classes[predicted[j]], *outputs[j])\n",
    "            break\n",
    "\n",
    "    # Log W&B model artifact\n",
    "    artifact.add(pred_table, \"cifar10_predictions\")\n",
    "    wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ff48f-8fe4-4eb1-a683-c74d9e8dc4d6",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "3b0ff48f-8fe4-4eb1-a683-c74d9e8dc4d6",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40"
    }
   },
   "source": [
    "# Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91010533-80af-4bb2-abc9-1467e70f6eb5",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "91010533-80af-4bb2-abc9-1467e70f6eb5",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "                'method': 'bayes',\n",
    "                'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
    "                'parameters': {\n",
    "                    'batch_size': {'values': [32, 128]},\n",
    "                    'epochs': {'value': 5},\n",
    "                    'lr': {'distribution': 'uniform',\n",
    "                                      'max': 1e-2,\n",
    "                                      'min': 1e-4},\n",
    "                    'model': {'values': ['ResNet18', 'ResNet34']}\n",
    "                    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27ef13-210b-407e-a6f1-2020429d1522",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "editing": false,
     "id": "3b27ef13-210b-407e-a6f1-2020429d1522",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(config = None):\n",
    "    with wandb.init(project='test-project', config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        trainloader, valloader, testloader = load_data(config)\n",
    "\n",
    "        if config['model']=='ResNet18':\n",
    "            model = resnet18(3,10)\n",
    "        else:\n",
    "            model = resnet34(3,10)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        step = 0\n",
    "        batch_checkpoint=50\n",
    "        epoch_durations = []\n",
    "        for epoch in range(config['epochs']):\n",
    "            \n",
    "            epoch_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            model.train()\n",
    "\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "            \n",
    "                # Move data to GPU if available \n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward + Backward + Optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Log every 50 batches\n",
    "                if i % batch_checkpoint == batch_checkpoint-1:\n",
    "                    step +=1\n",
    "                    print(f'epoch: {epoch + ((i+1)/len(trainloader)):.2f}')\n",
    "                    wandb.log({\"train_loss\": running_loss/batch_checkpoint, \"epoch\": epoch + ((i+1)/len(trainloader))}, step=step)\n",
    "                \n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, i + 1, running_loss / batch_checkpoint))\n",
    "                        \n",
    "                    running_loss = 0.0\n",
    "            \n",
    "            # Log at the end of each epoch\n",
    "            step +=1\n",
    "            print(f'epoch: {epoch + ((i+1)/len(trainloader)):.2f}')\n",
    "            wandb.log({\"train_loss\": running_loss/batch_checkpoint, \"epoch\": epoch + ((i+1)/len(trainloader))}, step=step)\n",
    "                \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                (epoch + 1, i + 1, running_loss / batch_checkpoint))\n",
    "\n",
    "            # Log validation metrics\n",
    "            val_loss, accuracy = validate_model(model, valloader, criterion, device)\n",
    "            wandb.log({\"val_loss\": val_loss, \"val_accuracy\": accuracy}, step=step)\n",
    "            print(f\"Valid Loss: {val_loss:3f}, accuracy: {accuracy:.2f}\")\n",
    "            \n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            wandb.log({\"epoch_runtime (seconds)\": epoch_duration}, step=step)\n",
    "\n",
    "            epoch_durations.append(epoch_duration)\n",
    "\n",
    "        avg_epoch_runtime = sum(epoch_durations) / len(epoch_durations)\n",
    "        wandb.log({\"avg epoch runtime (seconds)\": avg_epoch_runtime})\n",
    "\n",
    "        #Upload model artifact to Gradient and log model name to W&B\n",
    "        full_model_name = upload_model(config, model_client)\n",
    "        wandb.log({\"Notes\": full_model_name})\n",
    "\n",
    "    print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38bc28-6519-4bb5-9ef4-82cc7b1ec7dc",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "fc38bc28-6519-4bb5-9ef4-82cc7b1ec7dc",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"test-project\")\n",
    "wandb.agent(sweep_id, function=train, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b2aad-62f7-4412-9782-ca2f96ca0454",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "1b0b2aad-62f7-4412-9782-ca2f96ca0454",
     "kernelId": "9d1375dd-2f8b-4fc7-8a66-bf29b66e4e40",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
